---
title: Intriguing Ingress v2
date: 2021-02-07
tags: homelab, kubernetes
---

## Last time, on Janklab...

Earlier, I added a HAProxy based ingress controller, which went quite well.
Up until I got to the point where I needed to do multiple ingress classes.

As it turns out, HAProxy-Ingress doesn't support multiple ingress classes yet, you have to use the `haproxy.org/ingress.class` annotation (as of right now).
This is a little annoying for me, because this means I have to add an annotation for each ingress that I create and I can't default-select an ingress class.

So now I'm moving to traeflik. Such is the way of homelab. Like a migratory scavenger, it flies around, stopping only to consume the highest quality garbage it can.

## RBAC

Before, in the previous post, we created a fairly broad RBAC account and role for the ingress controller. I didn't really care back then, but here's a more cut down set of manifests for a more limited RBAC configuration.

In particular, this manifest removes a lot of the unnecessary permissions. The ingress service account now only has read access to Services, Endpoints, Secrets, IngressClass, and Ingress resources, and write access only to the Ingress.Status field of all of those.

I'm not too entirely happy about being able to read secrets, but that's part of being able to configure certificates. This could be mitigated by placing certificates in a specific namespace, but I'm a bit too lazy for that.

<details>
<summary>`ingress-rbac.yaml`</summary>
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ingress-service-account
  namespace: ingress
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ingress-cluster-role
rules:
  - apiGroups:
      - ""
    resources:
      - services
      - endpoints
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingresses
      - ingressclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - ingresses/status
    verbs:
      - update
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ingress-cluster-role-binding
  namespace: ingress
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-cluster-role
subjects:
- kind: ServiceAccount
  name: ingress-service-account
  namespace: ingress
```
</details>

## Ingress Classes

So, Ingress Classes were originally kinda sorta part of the standard. However, in Kubernetes 1.18+, you can specify the IngressClassName field in an Ingress resource instead of using the kubernetes.io/ingress.class annotation.

<div class="note">
Funny story, turns out that HAProxy Ingress isn't as up to date as traeflik ingress, so you need to use haproxy.org/ingress.class annotations instead of IngressClassName.
</div>

So, for the current setup I'm running, I have two ingress controllers. One for infrastructure (currently handling monitoring and what not), and one for internal services (things like plex, bookstack, etc).

Without further ado, here's the two ingress classes I care about at the moment.

<details>
<summary>`infrastructure-ingress-class.yaml`</summary>
```yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: infrastructure-ingress
spec: 
  controller: traefik.io/ingress-controller
```
</details>
<details>
<summary>`service-ingress-class.yaml`</summary>
```yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: service-ingress
  annotations:
    ingressclass.kubernetes.io/is-default-class: "true"
spec: 
  controller: traefik.io/ingress-controller
```
</details>

<div class="note">
Note that the service ingress class has the "is-default-class" annotation, which tells kubernetes that if we don't mention an specific Ingress Class, we should use this one.
</div>

## Ingress Controller

For the ingress controllers, it's mostly the same, but now we have to write a bit more configuration in the args section, as well as having two of them now (one for each ingress class)

<details>
<summary>`infrastructure-ingress-controller.yaml`</summary>
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: infrastructure-ingress
  name: infrastructure-ingress-controller
  namespace: ingress
spec:
  replicas: 1
  selector:
    matchLabels:
      app: infrastructure-ingress
  template:
    metadata:
      labels:
        app: infrastructure-ingress
    spec:
      serviceAccountName: ingress-service-account
      containers:
      - name: traefik-ingress
        image: traefik
        args:
          - --providers.kubernetesingress=true
          - --providers.kubernetesingress.ingressclass=infrastructure-ingress
          - --entryPoints.websecure.address=:443
          - --entryPoints.web.address=:80
          - --entrypoints.web.http.redirections.entryPoint.to=websecure
          - --entrypoints.web.http.redirections.entryPoint.scheme=https
          - --entrypoints.web.http.redirections.entrypoint.permanent=true
          - --serversTransport.insecureSkipVerify=true
          - --ping=true
          - --entryPoints.ping.address=:8082
          - --ping.entryPoint=ping
        securityContext:
          runAsUser:  1000
          runAsGroup: 1000
          capabilities:
            drop:
              - ALL
            add:
              - NET_BIND_SERVICE
        resources:
          requests:
            cpu: "500m"
            memory: "50Mi"
        livenessProbe:
          httpGet:
            path: /ping
            port: 8082
        ports:
        - name: https
          containerPort: 443
        - name: ping
          containerPort: 8082
        env:
        - name: TZ
          value: "America/New_York"
```
</details>
<details>
<summary>`service-ingress-controller.yaml`</summary>
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: service-ingress
  name: service-ingress-controller
  namespace: ingress
spec:
  replicas: 1
  selector:
    matchLabels:
      app: service-ingress
  template:
    metadata:
      labels:
        app: service-ingress
    spec:
      serviceAccountName: ingress-service-account
      containers:
      - name: traefik-ingress
        image: traefik
        args:
          - --providers.kubernetesingress=true
          - --providers.kubernetesingress.ingressclass=service-ingress
          - --entryPoints.websecure.address=:443
          - --entryPoints.web.address=:80
          - --entrypoints.web.http.redirections.entryPoint.to=websecure
          - --entrypoints.web.http.redirections.entryPoint.scheme=https
          - --entrypoints.web.http.redirections.entrypoint.permanent=true
          - --serversTransport.insecureSkipVerify=true
          - --ping=true
          - --entryPoints.ping.address=:8082
          - --ping.entryPoint=ping
        securityContext:
          runAsUser:  1000
          runAsGroup: 1000
          capabilities:
            drop:
              - ALL
            add:
              - NET_BIND_SERVICE
        resources:
          requests:
            cpu: "500m"
            memory: "50Mi"
        livenessProbe:
          httpGet:
            path: /ping
            port: 8082
        ports:
        - name: https
          containerPort: 443
        - name: ping
          containerPort: 8082
        env:
        - name: TZ
          value: "America/New_York"
```
</details>

The redirection config is there to force all clients to connect via HTTPS, and the ping is there to provide liveliness probes.
Note that the kubernetes ingress class name is specified in the command line arguments. This tells the ingress controller to only look for ingress resources that match the IngressClassName it has.

## Ingress Resources

So, the ingress resources stayed much the same, except an annotation does have to be added in order to allow the Traeflik HTTP Router to respond to queries on the HTTPS port.

For some reason, by default, if you have an ingress not annotated with `traefik.ingress.kubernetes.io/router.tls: "true"`, it *only* generates routes for HTTP, which means you just get a `404` page, as the routes don't exist on the `websecure` endpoint listening on `:443`.

<details>
<summary>`plex-ingress.yaml`</summary>
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: plex-ingress
  namespace: media
  annotations:
    traefik.ingress.kubernetes.io/router.tls: "true"
spec:
  rules:
  - host: plex.media.s.fnptr.net
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: plex
            port:
              number: 80
```
</details>

And hitting `plex.media.s.fnptr.net`, we get redirected to the HTTPS port automatically and into plex we go...
After a self-signed SSL certificate error.

## Securing Ingress

Of course, you don't want to see the certificate error *every* time you visit a site, and it's probably a good idea to use certificates to secure your ingresses.
There's two options here, you can either do traefik managed letsencrypt, upload your own certificates, or run some sort of external automated system for certificate management.

Unfortunately, traefik only supports single-ingress-controller letsencrypt unless you pay for the commercial, licensed version, which supports distributed letsencrypt, which limits our options to uploading our own certificates or using some sort of external automated system.
And let's be honest, nobody has the time to reupload certs every 90 days or so.

So, we can turn to [cert-manager](https://cert-manager.io/), which is installable via helm chart or operator. I personally installed the operator, but helm chart also works.
Once it's installed, it'll create some Custom Resource Definitions for certificates, the ones we care most about is the `ClusterIssuer` and the `Certificate` resources.

After uploading some cloudflare API tokens to kubernetes secrets, I can create a ClusterIssuer that will manage certificates for the cluster.

<details>
<summary>`letsencrypt-issuer.yaml`</summary>
```yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: cloudflare-acme-key
    solvers:
    - dns01:
        cloudflare:
          apiTokenSecretRef:
            name: cloudflare-api-token-secret
            key: api-token
      selector:
        dnsZones:
        - 'fnptr.net'
```
</details>

<div class="note">
I recommend using `https://acme-staging-v02.api.letsencrypt.org/directory`, the ACMEv2 server for testing before switching to the real server, since if you screw up, you might eat all 5 failed validation attempts per hour on the production letsencrypt ACMEv2 server.
</div>

After that's done, we can request our first certificate by writing out a Certificate resource. The `cert-manager` install will automatically detect that this certificate doesn't exist yet and try to issue it for us.
For our example, we'll request a cert for our monitoring stack at `monitor.fnptr.net`.

<details>
<summary>`monitor-cert.yaml`</summary>
```yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: monitor-cert
  namespace: infrastructure
spec:
  secretName: monitor-cert
  duration: 2160h # 90d
  renewBefore: 360h # 15d
  subject:
    organizations:
    - fnptr.net
  isCA: false
  privateKey:
    algorithm: RSA
    encoding: PKCS1
    size: 2048
  usages:
    - server auth
    - client auth
  dnsNames:
  - monitor.fnptr.net
  issuerRef:
    name: letsencrypt
    kind: ClusterIssuer
    group: cert-manager.io
```
</details>

Now we can finally create the ingress resource for grafana.

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  namespace: infrastructure
  annotations:
    traefik.ingress.kubernetes.io/router.tls: "true"
spec:
  ingressClassName: infrastructure-ingress
  tls:
  - hosts:
      - monitor.fnptr.net
    secretName: monitor-cert
  rules:
  - host: monitor.fnptr.net
    http:
      paths:
      - path: /grafana
        pathType: Prefix
        backend:
          service:
            name: grafana
            port:
              number: 80
```

Which selects the `infrastructure-ingress` ingress class and tells the ingress controller to load a certificate from a secret file named `monitor-cert`, which is automatically generated by our Certificate resource earlier.

<div class="info">
So, as it turns out, it's fine to share TLS configurations and certs on ingress that share a path...

But I make no guarantees about if /foo and /bar don't have the same tls configuration, no clue what happens. Didn't test, and I'm pretty sure I don't want to test it.
</div>

## ...And that's it

So, that's about it, everything works, everything's realtively nice.
Shame that I have to use an annotation to allow TLS ingress to happen, but a static annotation that never changes is marginally better than one that I need to remember values for.

There's obviously still rough spots that I should deal with, like figuring out how to ingress things that *aren't* HTTP without using services (ha, probably not), and adding more ingress classes.
But this works for right now, and it's mostly fine.

Mostly.